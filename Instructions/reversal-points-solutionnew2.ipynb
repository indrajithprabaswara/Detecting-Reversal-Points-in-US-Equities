{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "id":  "9777d6e7-bdbe-4c3f-b195-a31d73a69e60",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# Detecting Reversal Points in US Equities — Single-Notebook Solution\n",
                                     "\n",
                                     "This notebook is designed to run on Kaggle with the dataset imported as a Kaggle Dataset. It auto-detects files, builds strong tabular models with time-aware, ticker-aware cross-validation, and produces a valid submission CSV."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  1,
                      "id":  "777de949-827b-43bb-b6ca-15bc9ae838a6",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:15:00.222955Z",
                                                         "iopub.status.busy":  "2025-10-12T10:15:00.222575Z",
                                                         "iopub.status.idle":  "2025-10-12T10:15:11.077488Z",
                                                         "shell.execute_reply":  "2025-10-12T10:15:11.076302Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:15:00.222920Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Python 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
                                                       "pandas 2.2.3\n",
                                                       "numpy 1.26.4\n",
                                                       "scikit-learn 1.2.2\n",
                                                       "lightgbm 4.6.0\n",
                                                       "xgboost 2.0.3\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Setup: imports, versions, seeds\n",
                                     "import os\n",
                                     "import sys\n",
                                     "import gc\n",
                                     "from pathlib import Path\n",
                                     "import random\n",
                                     "import json\n",
                                     "import warnings\n",
                                     "warnings.filterwarnings(\u0027ignore\u0027)\n",
                                     "\n",
                                     "import numpy as np\n",
                                     "import pandas as pd\n",
                                     "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
                                     "try:\n",
                                     "    from sklearn.model_selection import StratifiedGroupKFold\n",
                                     "except Exception:\n",
                                     "    StratifiedGroupKFold = None\n",
                                     "from sklearn.preprocessing import LabelEncoder\n",
                                     "from sklearn.metrics import accuracy_score, f1_score\n",
                                     "from sklearn.impute import SimpleImputer\n",
                                     "from sklearn.linear_model import LogisticRegression\n",
                                     "from IPython.display import display\n",
                                     "\n",
                                     "# LightGBM / XGBoost (present on Kaggle)\n",
                                     "try:\n",
                                     "    import lightgbm as lgb\n",
                                     "except Exception as e:\n",
                                     "    lgb = None\n",
                                     "try:\n",
                                     "    from xgboost import XGBClassifier\n",
                                     "except Exception as e:\n",
                                     "    XGBClassifier = None\n",
                                     "\n",
                                     "SEED = 42\n",
                                     "random.seed(SEED); np.random.seed(SEED)\n",
                                     "\n",
                                     "def set_seed(seed=SEED):\n",
                                     "    random.seed(seed)\n",
                                     "    np.random.seed(seed)\n",
                                     "\n",
                                     "def display_versions():\n",
                                     "    print(\u0027Python\u0027, sys.version)\n",
                                     "    print(\u0027pandas\u0027, pd.__version__)\n",
                                     "    print(\u0027numpy\u0027, np.__version__)\n",
                                     "    try:\n",
                                     "        import sklearn\n",
                                     "        print(\u0027scikit-learn\u0027, sklearn.__version__)\n",
                                     "    except Exception:\n",
                                     "        pass\n",
                                     "    if lgb is not None:\n",
                                     "        print(\u0027lightgbm\u0027, lgb.__version__)\n",
                                     "    if XGBClassifier is not None:\n",
                                     "        import xgboost\n",
                                     "        print(\u0027xgboost\u0027, xgboost.__version__)\n",
                                     "\n",
                                     "display_versions()\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  2,
                      "id":  "c4380213-9d74-4ea7-8370-4e37ac8e860f",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:15:16.258827Z",
                                                         "iopub.status.busy":  "2025-10-12T10:15:16.257593Z",
                                                         "iopub.status.idle":  "2025-10-12T10:15:16.278448Z",
                                                         "shell.execute_reply":  "2025-10-12T10:15:16.277431Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:15:16.258769Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Using data directory: /kaggle/input/detecting-reversal-points-in-us-equities/competition_data\n",
                                                       "All required files present: train.csv test.csv sample_submission.csv\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "import os\n",
                                     "from pathlib import Path\n",
                                     "\n",
                                     "# Configuration: dataset name and path detection\n",
                                     "DATASET_NAME = os.environ.get(\u0027DATASET_NAME\u0027, \u0027detecting-reversal-points-in-us-equities\u0027)\n",
                                     "ROOT_INPUT = Path(f\"/kaggle/input/{DATASET_NAME}\")\n",
                                     "\n",
                                     "# Common Kaggle layout: many competition datasets put CSVs in a competition_data subfolder\n",
                                     "PREFERRED_SUBDIRS = [\n",
                                     "    ROOT_INPUT,\n",
                                     "    ROOT_INPUT / \u0027competition_data\u0027,\n",
                                     "    ROOT_INPUT / \u0027data\u0027,\n",
                                     "]\n",
                                     "\n",
                                     "# Local fallback options (keeps notebook runnable outside Kaggle)\n",
                                     "LOCAL_CANDIDATES = [\n",
                                     "    Path(\u0027./input\u0027),\n",
                                     "    Path(\u0027./data\u0027),\n",
                                     "    Path(\u0027.\u0027)\n",
                                     "]\n",
                                     "\n",
                                     "def resolve_dataset_path():\n",
                                     "    # Prefer Kaggle paths first\n",
                                     "    for p in PREFERRED_SUBDIRS:\n",
                                     "        if p.exists() and any(p.glob(\u0027*.csv\u0027)):\n",
                                     "            return p\n",
                                     "    # Then local fallbacks\n",
                                     "    for p in LOCAL_CANDIDATES:\n",
                                     "        if p.exists() and (p / \u0027train.csv\u0027).exists() and (p / \u0027test.csv\u0027).exists():\n",
                                     "            return p\n",
                                     "    # If not found, provide a helpful error with diagnostics\n",
                                     "    diagnostics = {\n",
                                     "        \"expected_root\": str(ROOT_INPUT),\n",
                                     "        \"checked_paths\": [str(x) for x in PREFERRED_SUBDIRS + LOCAL_CANDIDATES],\n",
                                     "    }\n",
                                     "    raise FileNotFoundError(\n",
                                     "        \"Dataset not found. Checked these paths: \"\n",
                                     "        f\"{diagnostics[\u0027checked_paths\u0027]}. \"\n",
                                     "        \"On Kaggle, ensure the dataset is added via Add Data and that CSVs exist. \"\n",
                                     "        f\"Root folder expected at: {diagnostics[\u0027expected_root\u0027]}\"\n",
                                     "    )\n",
                                     "\n",
                                     "DATA_DIR = resolve_dataset_path()\n",
                                     "print(\u0027Using data directory:\u0027, DATA_DIR)\n",
                                     "\n",
                                     "train_path = DATA_DIR / \u0027train.csv\u0027\n",
                                     "test_path = DATA_DIR / \u0027test.csv\u0027\n",
                                     "sample_path = DATA_DIR / \u0027sample_submission.csv\u0027\n",
                                     "\n",
                                     "missing = [str(p) for p in (train_path, test_path, sample_path) if not p.exists()]\n",
                                     "if missing:\n",
                                     "    print(\"Missing files (look here):\")\n",
                                     "    for m in missing:\n",
                                     "        print(\" -\", m)\n",
                                     "    # Show helpful listing for the dataset directory\n",
                                     "    print(\"\\nDirectory listing of DATA_DIR:\")\n",
                                     "    !ls -la \"{DATA_DIR}\" || true\n",
                                     "    raise FileNotFoundError(\"One or more required files are missing. Update DATA_DIR or move/unzip CSVs accordingly.\")\n",
                                     "else:\n",
                                     "    print(\"All required files present:\", train_path.name, test_path.name, sample_path.name)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  3,
                      "id":  "5a621c46-54a5-4dd8-9700-9e9bff2419cb",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:15:20.263491Z",
                                                         "iopub.status.busy":  "2025-10-12T10:15:20.263167Z",
                                                         "iopub.status.idle":  "2025-10-12T10:22:04.371210Z",
                                                         "shell.execute_reply":  "2025-10-12T10:22:04.369621Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:15:20.263465Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Train shape: (1932, 68507)\n",
                                                       "Test shape: (828, 68506)\n",
                                                       "Sample submission shape: (828, 2)\n",
                                                       "Target column: class_label\n",
                                                       "ID column: id\n",
                                                       "Ticker column: ticker_id\n",
                                                       "Time column: t\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Load data (robust to dtypes)\n",
                                     "date_cols = [\u0027t\u0027, \u0027date\u0027, \u0027timestamp\u0027]\n",
                                     "parse_dates = [c for c in date_cols if c in pd.read_csv(train_path, nrows=0).columns]\n",
                                     "\n",
                                     "train = pd.read_csv(train_path, parse_dates=parse_dates if parse_dates else None)\n",
                                     "test = pd.read_csv(test_path, parse_dates=parse_dates if parse_dates else None)\n",
                                     "sample_sub = pd.read_csv(sample_path)\n",
                                     "\n",
                                     "print(\u0027Train shape:\u0027, train.shape)\n",
                                     "print(\u0027Test shape:\u0027, test.shape)\n",
                                     "print(\u0027Sample submission shape:\u0027, sample_sub.shape)\n",
                                     "\n",
                                     "# Identify columns\n",
                                     "POSSIBLE_TARGETS = [\u0027class_label\u0027, \u0027target\u0027, \u0027label\u0027, \u0027y\u0027]\n",
                                     "target_col = None\n",
                                     "for c in POSSIBLE_TARGETS:\n",
                                     "    if c in train.columns:\n",
                                     "        target_col = c; break\n",
                                     "if target_col is None:\n",
                                     "    # Try from sample_sub second column name\n",
                                     "    if sample_sub.shape[1] \u003e= 2:\n",
                                     "        target_col = sample_sub.columns[1]\n",
                                     "    else:\n",
                                     "        raise RuntimeError(\u0027Cannot infer target column.\u0027)\n",
                                     "\n",
                                     "ID_CANDIDATES = [\u0027id\u0027, \u0027row_id\u0027, \u0027ID\u0027]\n",
                                     "id_col = None\n",
                                     "for c in ID_CANDIDATES:\n",
                                     "    if c in test.columns:\n",
                                     "        id_col = c; break\n",
                                     "if id_col is None:\n",
                                     "    # fallback: use index as id when not provided\n",
                                     "    id_col = \u0027id\u0027\n",
                                     "    test[id_col] = np.arange(len(test))\n",
                                     "\n",
                                     "TICKER_CANDIDATES = [\u0027ticker_id\u0027, \u0027ticker\u0027, \u0027symbol\u0027, \u0027stk\u0027, \u0027asset_id\u0027]\n",
                                     "ticker_col = None\n",
                                     "for c in TICKER_CANDIDATES:\n",
                                     "    if c in train.columns:\n",
                                     "        ticker_col = c; break\n",
                                     "\n",
                                     "TIME_CANDIDATES = [\u0027t\u0027, \u0027date\u0027, \u0027timestamp\u0027]\n",
                                     "time_col = None\n",
                                     "for c in TIME_CANDIDATES:\n",
                                     "    if c in train.columns:\n",
                                     "        time_col = c; break\n",
                                     "\n",
                                     "print(\u0027Target column:\u0027, target_col)\n",
                                     "print(\u0027ID column:\u0027, id_col)\n",
                                     "print(\u0027Ticker column:\u0027, ticker_col)\n",
                                     "print(\u0027Time column:\u0027, time_col)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  4,
                      "id":  "03434579-e484-408f-af92-7e8462aa4970",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:22:04.373202Z",
                                                         "iopub.status.busy":  "2025-10-12T10:22:04.372939Z",
                                                         "iopub.status.idle":  "2025-10-12T10:27:34.488352Z",
                                                         "shell.execute_reply":  "2025-10-12T10:27:34.487086Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:22:04.373182Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Mem usage 126.37 MB -\u003e 504.95 MB\n",
                                                       "Mem usage 54.13 MB -\u003e 216.38 MB\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Reduce memory (downcast numerics)\n",
                                     "def reduce_mem_usage(df, verbose=True):\n",
                                     "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
                                     "    for col in df.columns:\n",
                                     "        col_type = df[col].dtype\n",
                                     "        if pd.api.types.is_numeric_dtype(col_type):\n",
                                     "            c_min = df[col].min()\n",
                                     "            c_max = df[col].max()\n",
                                     "            if pd.api.types.is_integer_dtype(col_type):\n",
                                     "                if c_min \u003e= 0:\n",
                                     "                    if c_max \u003c 255:\n",
                                     "                        df[col] = df[col].astype(np.uint8)\n",
                                     "                    elif c_max \u003c 65535:\n",
                                     "                        df[col] = df[col].astype(np.uint16)\n",
                                     "                    elif c_max \u003c 4294967295:\n",
                                     "                        df[col] = df[col].astype(np.uint32)\n",
                                     "                    else:\n",
                                     "                        df[col] = df[col].astype(np.uint64)\n",
                                     "                else:\n",
                                     "                    if np.iinfo(np.int8).min \u003c c_min \u003c np.iinfo(np.int8).max:\n",
                                     "                        df[col] = df[col].astype(np.int8)\n",
                                     "                    elif np.iinfo(np.int16).min \u003c c_min \u003c np.iinfo(np.int16).max:\n",
                                     "                        df[col] = df[col].astype(np.int16)\n",
                                     "                    elif np.iinfo(np.int32).min \u003c c_min \u003c np.iinfo(np.int32).max:\n",
                                     "                        df[col] = df[col].astype(np.int32)\n",
                                     "                    else:\n",
                                     "                        df[col] = df[col].astype(np.int64)\n",
                                     "            else:\n",
                                     "                df[col] = df[col].astype(np.float32)\n",
                                     "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
                                     "    if verbose:\n",
                                     "        print(f\u0027Mem usage {start_mem:0.2f} MB -\u003e {end_mem:0.2f} MB\u0027)\n",
                                     "    return df\n",
                                     "\n",
                                     "train = reduce_mem_usage(train)\n",
                                     "test = reduce_mem_usage(test)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "72ad8511-eec0-4642-ae87-1d46a78ee2e5",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:27:34.489663Z",
                                                         "iopub.status.busy":  "2025-10-12T10:27:34.489394Z",
                                                         "iopub.status.idle":  "2025-10-12T10:27:34.869755Z",
                                                         "shell.execute_reply":  "2025-10-12T10:27:34.868251Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:27:34.489640Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Train head:\n"
                                                   ]
                                      },
                                      {
                                          "data":  {
                                                       "text/html":  [
                                                                         "\u003cdiv\u003e\n",
                                                                         "\u003cstyle scoped\u003e\n",
                                                                         "    .dataframe tbody tr th:only-of-type {\n",
                                                                         "        vertical-align: middle;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .dataframe tbody tr th {\n",
                                                                         "        vertical-align: top;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .dataframe thead th {\n",
                                                                         "        text-align: right;\n",
                                                                         "    }\n",
                                                                         "\u003c/style\u003e\n",
                                                                         "\u003ctable border=\"1\" class=\"dataframe\"\u003e\n",
                                                                         "  \u003cthead\u003e\n",
                                                                         "    \u003ctr style=\"text-align: right;\"\u003e\n",
                                                                         "      \u003cth\u003e\u003c/th\u003e\n",
                                                                         "      \u003cth\u003etrain_id\u003c/th\u003e\n",
                                                                         "      \u003cth\u003eticker_id\u003c/th\u003e\n",
                                                                         "      \u003cth\u003et\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ecross_threshold_from_above_100.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ecross_threshold_from_above_100.5\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ecross_threshold_from_above_101.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ecross_threshold_from_above_101.5\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ecross_threshold_from_above_102.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ecross_threshold_from_above_102.5\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ecross_threshold_from_above_103.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003e...\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_102.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_102.5\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_103.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_97.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_97.5\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_98.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_98.5\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_99.0\u003c/th\u003e\n",
                                                                         "      \u003cth\u003ezone_99.5\u003c/th\u003e\n",
                                                                         "      \u003cth\u003eclass_label\u003c/th\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "  \u003c/thead\u003e\n",
                                                                         "  \u003ctbody\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e0\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e2\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e2024-06-10\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e...\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003eNaN\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e1\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e1\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e3\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e2024-09-18\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e...\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003eNaN\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e2\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e2\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e6\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e2023-05-10\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e...\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003eNaN\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e3\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e3\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e3\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e2024-11-18\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e...\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003eNaN\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e4\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e4\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e2\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e2024-08-21\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e...\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003eNaN\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "  \u003c/tbody\u003e\n",
                                                                         "\u003c/table\u003e\n",
                                                                         "\u003cp\u003e5 rows × 68507 columns\u003c/p\u003e\n",
                                                                         "\u003c/div\u003e"
                                                                     ],
                                                       "text/plain":  [
                                                                          "   train_id  ticker_id          t  cross_threshold_from_above_100.0  \\\n",
                                                                          "0         0          2 2024-06-10                               0.0   \n",
                                                                          "1         1          3 2024-09-18                               0.0   \n",
                                                                          "2         2          6 2023-05-10                               0.0   \n",
                                                                          "3         3          3 2024-11-18                               0.0   \n",
                                                                          "4         4          2 2024-08-21                               0.0   \n",
                                                                          "\n",
                                                                          "   cross_threshold_from_above_100.5  cross_threshold_from_above_101.0  \\\n",
                                                                          "0                               0.0                               0.0   \n",
                                                                          "1                               0.0                               0.0   \n",
                                                                          "2                               0.0                               0.0   \n",
                                                                          "3                               0.0                               0.0   \n",
                                                                          "4                               0.0                               0.0   \n",
                                                                          "\n",
                                                                          "   cross_threshold_from_above_101.5  cross_threshold_from_above_102.0  \\\n",
                                                                          "0                               0.0                               0.0   \n",
                                                                          "1                               0.0                               0.0   \n",
                                                                          "2                               0.0                               0.0   \n",
                                                                          "3                               0.0                               0.0   \n",
                                                                          "4                               0.0                               0.0   \n",
                                                                          "\n",
                                                                          "   cross_threshold_from_above_102.5  cross_threshold_from_above_103.0  ...  \\\n",
                                                                          "0                               0.0                               0.0  ...   \n",
                                                                          "1                               0.0                               0.0  ...   \n",
                                                                          "2                               0.0                               0.0  ...   \n",
                                                                          "3                               0.0                               0.0  ...   \n",
                                                                          "4                               0.0                               0.0  ...   \n",
                                                                          "\n",
                                                                          "   zone_102.0  zone_102.5  zone_103.0  zone_97.0  zone_97.5  zone_98.0  \\\n",
                                                                          "0         0.0         0.0         0.0        0.0        0.0        0.0   \n",
                                                                          "1         0.0         0.0         0.0        0.0        1.0        1.0   \n",
                                                                          "2         0.0         0.0         0.0        0.0        0.0        0.0   \n",
                                                                          "3         0.0         0.0         0.0        0.0        0.0        0.0   \n",
                                                                          "4         1.0         1.0         0.0        0.0        0.0        0.0   \n",
                                                                          "\n",
                                                                          "   zone_98.5  zone_99.0  zone_99.5  class_label  \n",
                                                                          "0        1.0        1.0        0.0          NaN  \n",
                                                                          "1        0.0        0.0        0.0          NaN  \n",
                                                                          "2        1.0        1.0        0.0          NaN  \n",
                                                                          "3        0.0        0.0        0.0          NaN  \n",
                                                                          "4        0.0        0.0        0.0          NaN  \n",
                                                                          "\n",
                                                                          "[5 rows x 68507 columns]"
                                                                      ]
                                                   },
                                          "metadata":  {

                                                       },
                                          "output_type":  "display_data"
                                      },
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Classes: [\u0027HH\u0027, \u0027HL\u0027, \u0027LH\u0027, \u0027LL\u0027, \u0027nan\u0027]\n",
                                                       "Label mapping: {\u0027HH\u0027: 0, \u0027HL\u0027: 1, \u0027LH\u0027: 2, \u0027LL\u0027: 3, \u0027nan\u0027: 4}\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Basic overview and target encoding",
                                     "",
                                     "print(\u0027Train head:\u0027)",
                                     "",
                                     "display(train.head())",
                                     "",
                                     "",
                                     "",
                                     "y = train[target_col].copy()",
                                     "",
                                     "",
                                     "",
                                     "# Fill missing targets before converting to string to avoid literal \"nan\" appearing as a class",
                                     "",
                                     "y = y.fillna(\u0027None\u0027)",
                                     "",
                                     "y = y.astype(str)",
                                     "",
                                     "",
                                     "",
                                     "# Normalize label variants so only expected strings remain",
                                     "",
                                     "y = y.replace({",
                                     "",
                                     "    \u0027nan\u0027: \u0027None\u0027,",
                                     "",
                                     "    \u0027NaN\u0027: \u0027None\u0027,",
                                     "",
                                     "    \u0027\u0027: \u0027None\u0027,",
                                     "",
                                     "    \u0027NONE\u0027: \u0027None\u0027,",
                                     "",
                                     "    \u0027none\u0027: \u0027None\u0027,",
                                     "",
                                     "    \u0027HL\u0027: \u0027L\u0027,  # Map the pattern classes correctly",
                                     "",
                                     "    \u0027LL\u0027: \u0027L\u0027,",
                                     "",
                                     "    \u0027HH\u0027: \u0027H\u0027,",
                                     "",
                                     "    \u0027LH\u0027: \u0027H\u0027",
                                     "",
                                     "})",
                                     "",
                                     "",
                                     "",
                                     "EXPECTED_LABELS = [\u0027H\u0027, \u0027L\u0027, \u0027None\u0027]  # The official competition classes",
                                     "",
                                     "unique_labels = list(pd.unique(y))",
                                     "",
                                     "",
                                     "",
                                     "# Preserve expected ordering when present, append unexpected labels deterministically",
                                     "",
                                     "class_values = [lbl for lbl in EXPECTED_LABELS if lbl in unique_labels]",
                                     "",
                                     "extra_labels = sorted([lbl for lbl in unique_labels if lbl not in class_values])",
                                     "",
                                     "class_values.extend(extra_labels)",
                                     "",
                                     "",
                                     "",
                                     "# Ensure \"None\" is available for fallbacks",
                                     "",
                                     "if \u0027None\u0027 not in class_values:",
                                     "",
                                     "    class_values.append(\u0027None\u0027)",
                                     "",
                                     "",
                                     "",
                                     "print(\u0027Classes:\u0027, class_values)",
                                     "",
                                     "print(\u0027\\nClass Distribution:\u0027)",
                                     "",
                                     "class_counts = y.value_counts()",
                                     "",
                                     "print(class_counts)",
                                     "",
                                     "print(\u0027\\nClass Percentages:\u0027)",
                                     "",
                                     "print((class_counts / len(y) * 100).round(2), \u0027%\u0027)",
                                     "",
                                     "",
                                     "",
                                     "# Calculate balanced weights with stronger minority boost",
                                     "",
                                     "n_samples = len(y)",
                                     "",
                                     "class_counts_dict = y.value_counts().to_dict()",
                                     "",
                                     "# Scale the weights even more aggressively for extreme imbalance",
                                     "",
                                     "weight_multiplier = {",
                                     "",
                                     "    \u0027H\u0027: 15.0,  # Boost H class weight",
                                     "",
                                     "    \u0027L\u0027: 15.0,  # Boost L class weight",
                                     "",
                                     "    \u0027None\u0027: 1.0  # Keep None class as is",
                                     "",
                                     "}",
                                     "",
                                     "",
                                     "",
                                     "class_weight_dict = {",
                                     "",
                                     "    c: (n_samples / (len(class_counts_dict) * class_counts_dict[c])) * weight_multiplier[c]",
                                     "",
                                     "    for c in class_values",
                                     "",
                                     "}",
                                     "",
                                     "",
                                     "",
                                     "print(\u0027\\nComputed class weights:\u0027)",
                                     "",
                                     "print(class_weight_dict)",
                                     "",
                                     "",
                                     "",
                                     "# LabelEncoder for internal modeling order but preserve original labels",
                                     "",
                                     "le = LabelEncoder()",
                                     "",
                                     "le.fit(class_values)",
                                     "",
                                     "y_enc = le.transform(y)",
                                     "",
                                     "print(\u0027\\nLabel mapping:\u0027, dict(zip(le.classes_, le.transform(le.classes_))))",
                                     "\\nclass_weight_encoded = {le.transform([cls])[0]: class_weight_dict[cls] for cls in class_values}\\nprint(\u0027\\\\nEncoded class weights:\u0027, class_weight_encoded)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  6,
                      "id":  "46112e6d-235b-4749-9a38-cc986f4d7328",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:27:34.872468Z",
                                                         "iopub.status.busy":  "2025-10-12T10:27:34.872178Z",
                                                         "iopub.status.idle":  "2025-10-12T10:28:14.102217Z",
                                                         "shell.execute_reply":  "2025-10-12T10:28:14.100788Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:27:34.872447Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Numeric feature count: 68504 | Categorical: 0 | Total features kept: 68504\n",
                                                       "X shape: (1932, 68504) | X_test shape: (828, 68504)\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "import numpy as np\n",
                                     "import pandas as pd\n",
                                     "from pathlib import Path\n",
                                     "from sklearn.impute import SimpleImputer\n",
                                     "from sklearn.preprocessing import LabelEncoder\n",
                                     "from scipy import sparse\n",
                                     "\n",
                                     "# --- Assumes train, test, target_col, id_col, ticker_col, time_col are already defined ---\n",
                                     "\n",
                                     "# Build feature matrix: drop target, ids, and non-numeric object cols except limited categoricals\n",
                                     "drop_cols = set([target_col]) if target_col is not None else set()\n",
                                     "for c in [id_col, ticker_col, time_col]:\n",
                                     "    if c is not None and c in train.columns:\n",
                                     "        drop_cols.add(c)\n",
                                     "feat_cols = [c for c in train.columns if c not in drop_cols]\n",
                                     "\n",
                                     "# Identify categorical candidates among remaining features (based on TRAIN)\n",
                                     "cat_cols = []\n",
                                     "num_cols = []\n",
                                     "for c in feat_cols:\n",
                                     "    if train[c].dtype == \u0027object\u0027 or str(train[c].dtype).startswith(\u0027category\u0027):\n",
                                     "        cat_cols.append(c)\n",
                                     "    else:\n",
                                     "        num_cols.append(c)\n",
                                     "print(f\u0027Numeric feature count: {len(num_cols)} | Categorical: {len(cat_cols)} | Total features kept: {len(feat_cols)}\u0027)\n",
                                     "\n",
                                     "# Ensure TEST has the same numeric columns in the same order; missing cols will be filled with NaN\n",
                                     "# This avoids KeyError when test is missing columns present in train\n",
                                     "test_num = test.reindex(columns=num_cols)\n",
                                     "# Ensure TRAIN numeric DataFrame uses the same column order\n",
                                     "train_num = train.reindex(columns=num_cols)\n",
                                     "\n",
                                     "# Imputers\n",
                                     "num_imputer = SimpleImputer(strategy=\u0027median\u0027)\n",
                                     "cat_imputer = SimpleImputer(strategy=\u0027most_frequent\u0027)\n",
                                     "\n",
                                     "# Fit imputers on train numeric, transform test numeric\n",
                                     "X_num = num_imputer.fit_transform(train_num) if len(num_cols) else np.empty((len(train), 0))\n",
                                     "X_num_test = num_imputer.transform(test_num) if len(num_cols) else np.empty((len(test), 0))\n",
                                     "\n",
                                     "# Handle categoricals robustly: if a categorical column is missing in test, create an all-empty series for it\n",
                                     "if len(cat_cols):\n",
                                     "    for c in cat_cols:\n",
                                     "        train_series = train[c].astype(str).fillna(\u0027\u0027)\n",
                                     "        if c in test.columns:\n",
                                     "            test_series = test[c].astype(str).fillna(\u0027\u0027)\n",
                                     "        else:\n",
                                     "            test_series = pd.Series([\u0027\u0027] * len(test), index=test.index)\n",
                                     "        both = pd.concat([train_series, test_series], axis=0)\n",
                                     "        enc = LabelEncoder()\n",
                                     "        enc.fit(both)\n",
                                     "        train[c] = enc.transform(train_series)\n",
                                     "        test[c] = enc.transform(test_series)\n",
                                     "    # After encoding, reindex test/train to ensure same column order for cat_cols\n",
                                     "    train_cat_df = train.reindex(columns=cat_cols)\n",
                                     "    test_cat_df = test.reindex(columns=cat_cols)\n",
                                     "    X_cat = cat_imputer.fit_transform(train_cat_df)\n",
                                     "    X_cat_test = cat_imputer.transform(test_cat_df)\n",
                                     "else:\n",
                                     "    X_cat = np.empty((len(train), 0)); X_cat_test = np.empty((len(test), 0))\n",
                                     "\n",
                                     "# Combine numeric and categorical sparse matrices\n",
                                     "X = sparse.hstack([sparse.csr_matrix(X_num), sparse.csr_matrix(X_cat)], format=\u0027csr\u0027)\n",
                                     "X_test = sparse.hstack([sparse.csr_matrix(X_num_test), sparse.csr_matrix(X_cat_test)], format=\u0027csr\u0027)\n",
                                     "print(\u0027X shape:\u0027, X.shape, \u0027| X_test shape:\u0027, X_test.shape)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  7,
                      "id":  "8feb2ab5-2361-4a01-b73f-ccd325080be6",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:28:14.103620Z",
                                                         "iopub.status.busy":  "2025-10-12T10:28:14.103243Z",
                                                         "iopub.status.idle":  "2025-10-12T10:28:14.116035Z",
                                                         "shell.execute_reply":  "2025-10-12T10:28:14.114254Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:28:14.103550Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Class weights (inverse freq): {0: 9.66, 1: 13.8, 2: 21.466666666666665, 3: 14.861538461538462, 4: 0.2123076923076923}\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Time-aware, ticker-aware cross-validation splitter\n",
                                     "def get_folds(n_splits=5, shuffle=False):\n",
                                     "    # Prefer group-wise stratification by ticker if present\n",
                                     "    if ticker_col and ticker_col in train.columns:\n",
                                     "        groups = train[ticker_col].astype(str).values\n",
                                     "    else:\n",
                                     "        groups = None\n",
                                     "    if StratifiedGroupKFold is not None and groups is not None:\n",
                                     "        return StratifiedGroupKFold(n_splits=n_splits, shuffle=shuffle, random_state=SEED).split(X, y_enc, groups)\n",
                                     "    if groups is not None:\n",
                                     "        # Fall back to GroupKFold without stratification\n",
                                     "        gkf = GroupKFold(n_splits=n_splits)\n",
                                     "        return gkf.split(X, y_enc, groups)\n",
                                     "    # Final fallback: stratified by y only\n",
                                     "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=SEED)\n",
                                     "    return skf.split(X, y_enc)\n",
                                     "\n",
                                     "def class_weights_from_y(y_vals):\n",
                                     "    # Inverse frequency weights to handle imbalance in some learners\n",
                                     "    classes, counts = np.unique(y_vals, return_counts=True)\n",
                                     "    total = counts.sum()\n",
                                     "    weights = {c: total / (len(classes) * cnt) for c, cnt in zip(classes, counts)}\n",
                                     "    return weights\n",
                                     "\n",
                                     "cw_map = class_weights_from_y(y_enc)\n",
                                     "print(\u0027Class weights (inverse freq):\u0027, cw_map)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "0db5cd62-a959-4fc6-9e9d-c2022f5b9b89",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:28:14.118001Z",
                                                         "iopub.status.busy":  "2025-10-12T10:28:14.117584Z",
                                                         "iopub.status.idle":  "2025-10-12T10:28:14.144287Z",
                                                         "shell.execute_reply":  "2025-10-12T10:28:14.143086Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:28:14.117974Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# Modeling utilities: LightGBM and Logistic baselines; hierarchical classifier for accuracy\n",
                                     "def train_lgbm(X_tr, y_tr, X_va=None, y_va=None, num_class=3, params=None):\n",
                                     "    if lgb is None:\n",
                                     "        return None\n",
                                     "    default_params = dict(\n",
                                     "        objective=\u0027multiclass\u0027,\n",
                                     "        num_class=num_class,\n",
                                     "        boosting_type=\u0027gbdt\u0027,\n",
                                     "        learning_rate=0.05,\n",
                                     "        n_estimators=1000,\n",
                                     "        subsample=0.9,\n",
                                     "        colsample_bytree=0.2,\n",
                                     "        reg_alpha=1.0,\n",
                                     "        reg_lambda=2.0,\n",
                                     "        max_depth=-1,\n",
                                     "        num_leaves=63,\n",
                                     "        min_data_in_leaf=20,\n",
                                     "        random_state=SEED,\n",
                                     "        n_jobs=-1\n",
                                     "    )\n",
                                     "    if params: default_params.update(params)\n",
                                     "    model = lgb.LGBMClassifier(**default_params)\n",
                                     "    if X_va is not None and y_va is not None:\n",
                                     "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\u0027multi_logloss\u0027,\n",
                                     "                  callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
                                     "    else:\n",
                                     "        model.fit(X_tr, y_tr)\n",
                                     "    return model\n",
                                     "\n",
                                     "def train_logreg(X_tr, y_tr, X_va=None, y_va=None, num_class=3):\n",
                                     "    # Multinomial logistic with strong regularization for high-dim sparse\n",
                                     "    solver = \u0027saga\u0027\n",
                                     "    C = 1.0\n",
                                     "    max_iter = 300\n",
                                     "    model = LogisticRegression(\n",
                                     "        multi_class=\u0027multinomial\u0027,\n",
                                     "        solver=solver,\n",
                                     "        C=C,\n",
                                     "        max_iter=max_iter,\n",
                                     "        class_weight=\u0027balanced\u0027,\n",
                                     "        n_jobs=-1,\n",
                                     "        random_state=SEED\n",
                                     "    )\n",
                                     "    model.fit(X_tr, y_tr)\n",
                                     "    return model\n",
                                     "\n",
                                     "def evaluate_preds(y_true, y_pred, average=\u0027macro\u0027):\n",
                                     "    acc = accuracy_score(y_true, y_pred)\n",
                                     "    f1 = f1_score(y_true, y_pred, average=average)\n",
                                     "    return acc, f1\n",
                                     "\n",
                                     "def sigmoid(x):\n",
                                     "    return 1 / (1 + np.exp(-x))\n",
                                     "\n",
                                     "def hierarchical_fit_predict(X, y_enc, folds, classes_order, favor_accuracy=True):\n",
                                     "    # Step 1: Binary model None vs {H,L} using logistic regression (robust for sparse high-dim)\n",
                                     "    # Identify encodings\n",
                                     "    idx_none = np.where(classes_order == \u0027None\u0027)[0]\n",
                                     "    if len(idx_none) == 0 or len(classes_order) \u003c 3:\n",
                                     "        # If \u0027None\u0027 absent or not enough classes, revert to multiclass training\n",
                                     "        return None\n",
                                     "    none_label_enc = idx_none[0]\n",
                                     "    y_is_none = (y_enc == none_label_enc).astype(int)\n",
                                     "\n",
                                     "    thresholds = []\n",
                                     "    fold_metrics = []\n",
                                     "    for fold, (tr_idx, va_idx) in enumerate(folds):\n",
                                     "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
                                     "        y_tr_bin, y_va_bin = y_is_none[tr_idx], y_is_none[va_idx]\n",
                                     "        # Binary logistic with strong regularization\n",
                                     "        clf_none = LogisticRegression(\n",
                                     "            solver=\u0027saga\u0027,\n",
                                     "            penalty=\u0027l2\u0027,\n",
                                     "            C=1.0,\n",
                                     "            max_iter=300,\n",
                                     "            class_weight=\u0027balanced\u0027,\n",
                                     "            n_jobs=-1,\n",
                                     "            random_state=SEED\n",
                                     "        )\n",
                                     "        clf_none.fit(X_tr, y_tr_bin)\n",
                                     "        # Decision scores/proba for validation\n",
                                     "        if hasattr(clf_none, \u0027predict_proba\u0027):\n",
                                     "            p_none = clf_none.predict_proba(X_va)[:,1]\n",
                                     "        else:\n",
                                     "            p_none = sigmoid(clf_none.decision_function(X_va))\n",
                                     "        # Tune threshold to maximize accuracy on validation\n",
                                     "        best_thr, best_acc = 0.5, -1\n",
                                     "        for thr in np.linspace(0.5, 0.99, 50):\n",
                                     "            pred_is_none = (p_none \u003e= thr).astype(int)\n",
                                     "            # Provisional accuracy for binary decision (None vs non-None)\n",
                                     "            acc_tmp = (pred_is_none == y_va_bin).mean()\n",
                                     "            if acc_tmp \u003e best_acc:\n",
                                     "                best_acc, best_thr = acc_tmp, thr\n",
                                     "        thresholds.append(best_thr)\n",
                                     "        fold_metrics.append({\u0027fold\u0027: fold, \u0027thr\u0027: best_thr, \u0027acc_tmp\u0027: best_acc})\n",
                                     "    thr_global = float(np.median(thresholds)) if favor_accuracy else float(np.mean(thresholds))\n",
                                     "    return {\u0027thr\u0027: thr_global}  # store what we need for inference\n",
                                     "\n",
                                     "def hierarchical_predict_test(X_train, y_enc, X_test, classes_order, thr_cfg):\n",
                                     "    # Fit final binary \u0027None\u0027 classifier on full data\n",
                                     "    idx_none = np.where(classes_order == \u0027None\u0027)[0]\n",
                                     "    none_label_enc = idx_none[0]\n",
                                     "    y_is_none = (y_enc == none_label_enc).astype(int)\n",
                                     "    clf_none = LogisticRegression(\n",
                                     "        solver=\u0027saga\u0027, penalty=\u0027l2\u0027, C=1.0, max_iter=400, class_weight=\u0027balanced\u0027, n_jobs=-1, random_state=SEED\n",
                                     "    )\n",
                                     "    clf_none.fit(X_train, y_is_none)\n",
                                     "    if hasattr(clf_none, \u0027predict_proba\u0027):\n",
                                     "        p_none_test = clf_none.predict_proba(X_test)[:,1]\n",
                                     "    else:\n",
                                     "        p_none_test = sigmoid(clf_none.decision_function(X_test))\n",
                                     "    thr = thr_cfg[\u0027thr\u0027]\n",
                                     "    is_none_pred = (p_none_test \u003e= thr)\n",
                                     "    # For non-None, train a 2-class model on H vs L only\n",
                                     "    mask_hl = (y_enc != none_label_enc)\n",
                                     "    y_hl = y_enc[mask_hl]\n",
                                     "    # Remap encodings of remaining classes to [0,1,...] for n-class\n",
                                     "    classes_hl = sorted([c for c in range(len(classes_order)) if c != none_label_enc])\n",
                                     "    enc_map = {orig_label: i for i, orig_label in enumerate(classes_hl)}\n",
                                     "    y_hl_bin = np.array([enc_map[v] for v in y_hl], dtype=int)\n",
                                     "    X_hl = X_train[mask_hl]\n",
                                     "    # Use a fast linear model for HL split\n",
                                     "    clf_hl = LogisticRegression(\n",
                                     "        solver=\u0027saga\u0027, penalty=\u0027l2\u0027, C=1.0, max_iter=400, class_weight=\u0027balanced\u0027, n_jobs=-1, random_state=SEED\n",
                                     "    )\n",
                                     "    clf_hl.fit(X_hl, y_hl_bin)\n",
                                     "    # Predict HL only where non-None\n",
                                     "    pred = np.full(X_test.shape[0], none_label_enc, dtype=int)\n",
                                     "    idx_non_none = np.where(~is_none_pred)[0]\n",
                                     "    if idx_non_none.size \u003e 0:\n",
                                     "        proba_hl = clf_hl.predict_proba(X_test[idx_non_none])\n",
                                     "        hl_pred = proba_hl.argmax(axis=1)\n",
                                     "        # Map back to original encodings\n",
                                     "        rev_map = {i: orig_label for i, orig_label in enumerate(classes_hl)}\n",
                                     "        pred[idx_non_none] = np.array([rev_map[v] for v in hl_pred], dtype=int)\n",
                                     "    return pred\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "571c0b39-377d-48a0-8af3-814e9b51aed3",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:28:14.146094Z",
                                                         "iopub.status.busy":  "2025-10-12T10:28:14.145793Z",
                                                         "iopub.status.idle":  "2025-10-12T10:28:19.964088Z",
                                                         "shell.execute_reply":  "2025-10-12T10:28:19.962824Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:28:14.146073Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "[LGBM] Training failed with exception: TypeError(\"train() got an unexpected keyword argument \u0027early_stopping_rounds\u0027\")\n",
                                                       "[LGBM] Fold 0: model failed; using majority-class fallback acc=0.95497 f1_macro=0.19539\n",
                                                       "[LGBM] Training failed with exception: TypeError(\"train() got an unexpected keyword argument \u0027early_stopping_rounds\u0027\")\n",
                                                       "[LGBM] Fold 1: model failed; using majority-class fallback acc=0.93478 f1_macro=0.19326\n",
                                                       "[LGBM] Training failed with exception: TypeError(\"train() got an unexpected keyword argument \u0027early_stopping_rounds\u0027\")\n",
                                                       "[LGBM] Fold 2: model failed; using majority-class fallback acc=0.94410 f1_macro=0.19425\n",
                                                       "[LGBM] Training failed with exception: TypeError(\"train() got an unexpected keyword argument \u0027early_stopping_rounds\u0027\")\n",
                                                       "[LGBM] Fold 3: model failed; using majority-class fallback acc=0.92857 f1_macro=0.19259\n",
                                                       "[LGBM] Training failed with exception: TypeError(\"train() got an unexpected keyword argument \u0027early_stopping_rounds\u0027\")\n",
                                                       "[LGBM] Fold 4: model failed; using majority-class fallback acc=0.93478 f1_macro=0.19326\n",
                                                       "[LGBM] CV mean acc: 0.9394409937888198 | f1_macro: 0.19375036339700452\n",
                                                       "Hierarchical approach not applicable (no None class detected).\n",
                                                       "CV results: {\n",
                                                       "  \"lgbm\": {\n",
                                                       "    \"acc\": 0.9394409937888198,\n",
                                                       "    \"f1_macro\": 0.19375036339700452\n",
                                                       "  }\n",
                                                       "}\n",
                                                       "Selected approach: lgbm\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Fixed CV block for LightGBM + hierarchical fallback\n",
                                     "# - uses stable params for small-data situations\n",
                                     "# - forces row-wise to avoid the multi-threading warning overhead\n",
                                     "# - uses predict_proba + argmax for multiclass\n",
                                     "# - protects against models that fail to split (returns fallback predictions)\n",
                                     "# - frees memory between folds and logs cleanly\n",
                                     "\n",
                                     "import gc\n",
                                     "import json\n",
                                     "import numpy as np\n",
                                     "import lightgbm as lgb\n",
                                     "from sklearn.metrics import accuracy_score, f1_score\n",
                                     "from tqdm.auto import tqdm\n",
                                     "\n",
                                     "# Helper: safe train wrapper that returns None on catastrophic failure\n",
                                     "def safe_train_lgbm(X_tr, y_tr, X_va, y_va, num_class, params=None):\n",
                                     "    if params is None:\n",
                                     "        params = {\n",
                                     "            \"objective\": \"multiclass\",\n",
                                     "            \"num_class\": num_class,\n",
                                     "            \"metric\": \"multi_logloss\",\n",
                                     "            \"verbosity\": -1,\n",
                                     "            \"boosting_type\": \"gbdt\",\n",
                                     "            \"learning_rate\": 0.05,\n",
                                     "            \"num_leaves\": 64,\n",
                                     "            \"min_data_in_leaf\": 20,\n",
                                     "            # Force row-wise to avoid row/colwise auto-choice overhead message\n",
                                     "            \"force_row_wise\": True,\n",
                                     "            # Add class weights to handle imbalance\n",
                                     "            \"is_unbalance\": True,\n",
                                     "            # Reduce risk of \"no further splits\" by allowing small child samples when dataset tiny\n",
                                     "            \"min_child_samples\": 5,\n",
                                     "            \"seed\": 42,\n",
                                     "        }\n",
                                     "    try:\n",
                                     "        # Calculate class weights for handling imbalance\n",
                                     "        class_counts = np.bincount(y_tr)\n",
                                     "        class_weights = len(y_tr) / (num_class * class_counts)\n",
                                     "        sample_weights_tr = class_weights[y_tr]\n",
                                     "        sample_weights_va = class_weights[y_va]\n",
                                     "        \n",
                                     "        dtrain = lgb.Dataset(X_tr, label=y_tr, weight=sample_weights_tr)\n",
                                     "        dvalid = lgb.Dataset(X_va, label=y_va, weight=sample_weights_va, reference=dtrain)\n",
                                     "        \n",
                                     "        # Train with early stopping\n",
                                     "        model = lgb.train(\n",
                                     "            params,\n",
                                     "            dtrain,\n",
                                     "            num_boost_round=2000,\n",
                                     "            valid_sets=[dvalid],\n",
                                     "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
                                     "        )\n",
                                     "        return model\n",
                                     "    except Exception as e:\n",
                                     "        print(\"[LGBM] Training failed with exception:\", repr(e))\n",
                                     "        return None\n",
                                     "\n",
                                     "# Evaluate helpers\n",
                                     "def evaluate_preds(y_true, y_pred):\n",
                                     "    # Accept both class labels and prob arrays\n",
                                     "    if y_pred.ndim == 2:\n",
                                     "        y_hat = np.argmax(y_pred, axis=1)\n",
                                     "    else:\n",
                                     "        y_hat = np.asarray(y_pred).astype(int)\n",
                                     "    acc = accuracy_score(y_true, y_hat)\n",
                                     "    f1m = f1_score(y_true, y_hat, average=\u0027macro\u0027, zero_division=0)\n",
                                     "    return acc, f1m\n",
                                     "\n",
                                     "# Decide number of splits safely\n",
                                     "n_splits = 5 if (len(np.unique(y_enc)) \u003e= 3 and len(y_enc) \u003e= 100) else 3\n",
                                     "splits = list(get_folds(n_splits=n_splits, shuffle=True))\n",
                                     "\n",
                                     "cv_results = {}\n",
                                     "\n",
                                     "# LightGBM multiclass CV (if available)\n",
                                     "if \u0027lgb\u0027 in globals() and lgb is not None:\n",
                                     "    lgb_acc, lgb_f1 = [], []\n",
                                     "    for fold, (tr_idx, va_idx) in enumerate(splits):\n",
                                     "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
                                     "        y_tr, y_va = y_enc[tr_idx], y_enc[va_idx]\n",
                                     "\n",
                                     "        model = safe_train_lgbm(X_tr, y_tr, X_va, y_va, num_class=len(le.classes_))\n",
                                     "\n",
                                     "        if model is None:\n",
                                     "            # Fallback: predict majority class if model failed\n",
                                     "            majority = np.bincount(y_tr).argmax()\n",
                                     "            y_pred = np.full(len(y_va), majority, dtype=int)\n",
                                     "            # convert to 2D prob-like for evaluate_preds compatibility\n",
                                     "            y_pred_probs = np.eye(len(le.classes_))[y_pred]\n",
                                     "            acc, f1m = evaluate_preds(y_va, y_pred_probs)\n",
                                     "            print(f\u0027[LGBM] Fold {fold}: model failed; using majority-class fallback acc={acc:.5f} f1_macro={f1m:.5f}\u0027)\n",
                                     "        else:\n",
                                     "            # predict_proba then argmax (works for sparse matrices returned by X)\n",
                                     "            try:\n",
                                     "                y_pred_probs = model.predict(X_va)\n",
                                     "                # some lgb versions return shape (n_samples,) for binary; normalize to 2D\n",
                                     "                if y_pred_probs.ndim == 1:\n",
                                     "                    y_pred_probs = np.vstack([1 - y_pred_probs, y_pred_probs]).T\n",
                                     "                \n",
                                     "                # Apply confidence threshold for None class\n",
                                     "                none_idx = np.where(le.classes_ == \u0027None\u0027)[0][0]\n",
                                     "                conf_threshold = 0.4  # Adjust based on validation performance\n",
                                     "                max_probs = np.max(y_pred_probs, axis=1)\n",
                                     "                uncertain_mask = max_probs \u003c conf_threshold\n",
                                     "                \n",
                                     "                y_pred = np.argmax(y_pred_probs, axis=1)\n",
                                     "                # Assign uncertain predictions to None class\n",
                                     "                y_pred[uncertain_mask] = none_idx\n",
                                     "                \n",
                                     "                acc, f1m = evaluate_preds(y_va, y_pred)\n",
                                     "                print(f\u0027[LGBM] Fold {fold}: acc={acc:.5f} f1_macro={f1m:.5f}\u0027)\n",
                                     "                \n",
                                     "                # Print confusion matrix for this fold\n",
                                     "                from sklearn.metrics import confusion_matrix\n",
                                     "                cm = confusion_matrix(y_va, y_pred)\n",
                                     "                print(f\u0027\\nConfusion Matrix for Fold {fold}:\u0027)\n",
                                     "                print(\u0027True labels (rows) vs Predicted labels (columns)\u0027)\n",
                                     "                print(\u0027Labels:\u0027, le.classes_)\n",
                                     "                print(cm)\n",
                                     "                \n",
                                     "            except Exception as e:\n",
                                     "                print(\"[LGBM] Prediction failed:\", repr(e))\n",
                                     "                majority = np.bincount(y_tr).argmax()\n",
                                     "                y_pred_probs = np.eye(len(le.classes_))[np.full(len(y_va), majority)]\n",
                                     "                acc, f1m = evaluate_preds(y_va, y_pred_probs)\n",
                                     "                print(f\u0027[LGBM] Fold {fold}: prediction failed, using majority fallback acc={acc:.5f} f1_macro={f1m:.5f}\u0027)\n",
                                     "            \n",
                                     "            # free model ASAP to reduce memory footprint\n",
                                     "            del model\n",
                                     "\n",
                                     "        lgb_acc.append(acc)\n",
                                     "        lgb_f1.append(f1m)\n",
                                     "        gc.collect()\n",
                                     "\n",
                                     "    cv_results[\u0027lgbm\u0027] = {\u0027acc\u0027: float(np.mean(lgb_acc)), \u0027f1_macro\u0027: float(np.mean(lgb_f1))}\n",
                                     "    print(\u0027\\n[LGBM] CV Summary:\u0027)\n",
                                     "    print(\u0027Mean accuracy:\u0027, cv_results[\u0027lgbm\u0027][\u0027acc\u0027])\n",
                                     "    print(\u0027Mean F1-macro:\u0027, cv_results[\u0027lgbm\u0027][\u0027f1_macro\u0027])\n",
                                     "    print(\u0027Std accuracy:\u0027, float(np.std(lgb_acc)))\n",
                                     "    print(\u0027Std F1-macro:\u0027, float(np.std(lgb_f1)))\n",
                                     "else:\n",
                                     "    print(\u0027LightGBM not available; skipping.\u0027)\n",
                                     "\n",
                                     "# Hierarchical approach CV (optimize accuracy)\n",
                                     "classes_order = le.classes_\n",
                                     "hier_cfg = hierarchical_fit_predict(X, y_enc, splits, classes_order, favor_accuracy=True)\n",
                                     "if hier_cfg is not None:\n",
                                     "    accs, f1s = [], []\n",
                                     "    for fold, (tr_idx, va_idx) in enumerate(splits):\n",
                                     "        # Fit on train split only (use simple split wrapper: here we fit on tr_idx and predict va_idx)\n",
                                     "        cfg_fold = hierarchical_fit_predict(X[tr_idx], y_enc[tr_idx], [(np.arange(len(tr_idx)), np.arange(len(tr_idx)))], classes_order)\n",
                                     "        y_pred_va = hierarchical_predict_test(X[tr_idx], y_enc[tr_idx], X[va_idx], classes_order, cfg_fold)\n",
                                     "        acc, f1m = evaluate_preds(y_enc[va_idx], y_pred_va)\n",
                                     "        accs.append(acc); f1s.append(f1m)\n",
                                     "        print(f\u0027[HIER] Fold {fold}: acc={acc:.5f} f1_macro={f1m:.5f}\u0027)\n",
                                     "        \n",
                                     "        # Print confusion matrix for this fold\n",
                                     "        from sklearn.metrics import confusion_matrix\n",
                                     "        cm = confusion_matrix(y_enc[va_idx], y_pred_va)\n",
                                     "        print(f\u0027\\nConfusion Matrix for Fold {fold}:\u0027)\n",
                                     "        print(\u0027True labels (rows) vs Predicted labels (columns)\u0027)\n",
                                     "        print(\u0027Labels:\u0027, le.classes_)\n",
                                     "        print(cm)\n",
                                     "        \n",
                                     "    cv_results[\u0027hier\u0027] = {\u0027acc\u0027: float(np.mean(accs)), \u0027f1_macro\u0027: float(np.mean(f1s)), \u0027cfg\u0027: hier_cfg}\n",
                                     "    print(\u0027\\n[HIER] CV Summary:\u0027)\n",
                                     "    print(\u0027Mean accuracy:\u0027, cv_results[\u0027hier\u0027][\u0027acc\u0027])\n",
                                     "    print(\u0027Mean F1-macro:\u0027, cv_results[\u0027hier\u0027][\u0027f1_macro\u0027])\n",
                                     "    print(\u0027Std accuracy:\u0027, float(np.std(accs)))\n",
                                     "    print(\u0027Std F1-macro:\u0027, float(np.std(f1s)))\n",
                                     "else:\n",
                                     "    print(\u0027Hierarchical approach not applicable (no None class detected).\u0027)\n",
                                     "\n",
                                     "print(\u0027\\nCV results:\u0027, json.dumps(cv_results, indent=2))\n",
                                     "\n",
                                     "# Choose best by F1-macro instead of accuracy to handle class imbalance better\n",
                                     "best_key = max(cv_results.keys(), key=lambda k: cv_results[k][\u0027f1_macro\u0027]) if cv_results else None\n",
                                     "print(\u0027Selected approach:\u0027, best_key)\n",
                                     "if best_key is None:\n",
                                     "    raise RuntimeError(\u0027No model trained during CV.\u0027)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "859a0eb8-f055-4c2b-8f4b-d0a42bb8f29b",
                      "metadata":  {
                                       "execution":  {
                                                         "iopub.execute_input":  "2025-10-12T10:28:19.965750Z",
                                                         "iopub.status.busy":  "2025-10-12T10:28:19.965446Z",
                                                         "iopub.status.idle":  "2025-10-12T10:28:50.035022Z",
                                                         "shell.execute_reply":  "2025-10-12T10:28:50.032885Z",
                                                         "shell.execute_reply.started":  "2025-10-12T10:28:19.965711Z"
                                                     },
                                       "trusted":  true
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "[FINAL] Training LightGBM on full data...\n",
                                                       "[LGBM] Final training failed: TypeError(\"train() got an unexpected keyword argument \u0027early_stopping_rounds\u0027\")\n",
                                                       "[FINAL] LGBM training failed; falling back to logistic regression.\n",
                                                       "Saved submission to /kaggle/working/submission.csv\n"
                                                   ]
                                      },
                                      {
                                          "data":  {
                                                       "text/html":  [
                                                                         "\u003cdiv\u003e\n",
                                                                         "\u003cstyle scoped\u003e\n",
                                                                         "    .dataframe tbody tr th:only-of-type {\n",
                                                                         "        vertical-align: middle;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .dataframe tbody tr th {\n",
                                                                         "        vertical-align: top;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .dataframe thead th {\n",
                                                                         "        text-align: right;\n",
                                                                         "    }\n",
                                                                         "\u003c/style\u003e\n",
                                                                         "\u003ctable border=\"1\" class=\"dataframe\"\u003e\n",
                                                                         "  \u003cthead\u003e\n",
                                                                         "    \u003ctr style=\"text-align: right;\"\u003e\n",
                                                                         "      \u003cth\u003e\u003c/th\u003e\n",
                                                                         "      \u003cth\u003eid\u003c/th\u003e\n",
                                                                         "      \u003cth\u003eclass_label\u003c/th\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "  \u003c/thead\u003e\n",
                                                                         "  \u003ctbody\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e0\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003enan\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e1\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e1\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003enan\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e2\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e2\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003enan\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e3\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e3\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003enan\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e4\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003e4\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003enan\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "  \u003c/tbody\u003e\n",
                                                                         "\u003c/table\u003e\n",
                                                                         "\u003c/div\u003e"
                                                                     ],
                                                       "text/plain":  [
                                                                          "   id class_label\n",
                                                                          "0   0         nan\n",
                                                                          "1   1         nan\n",
                                                                          "2   2         nan\n",
                                                                          "3   3         nan\n",
                                                                          "4   4         nan"
                                                                      ]
                                                   },
                                          "metadata":  {

                                                       },
                                          "output_type":  "display_data"
                                      }
                                  ],
                      "source":  [
                                     "import gc\n",
                                     "import numpy as np\n",
                                     "import pandas as pd\n",
                                     "from pathlib import Path\n",
                                     "from sklearn.linear_model import LogisticRegression\n",
                                     "\n",
                                     "# Safe LightGBM trainer for final model (returns model or None on failure)\n",
                                     "def safe_train_lgbm_final(X_train, y_train, num_class, params=None):\n",
                                     "    try:\n",
                                     "        if params is None:\n",
                                     "            params = {\n",
                                     "                \"objective\": \"multiclass\",\n",
                                     "                \"num_class\": num_class,\n",
                                     "                \"metric\": [\"multi_logloss\", \"multi_error\"],\n",
                                     "                \"verbosity\": -1,\n",
                                     "                \"boosting_type\": \"gbdt\",\n",
                                     "                \"learning_rate\": 0.02,  # Increased for better convergence\n",
                                     "                \"num_leaves\": 31,  # Increased for more complex patterns\n",
                                     "                \"min_data_in_leaf\": 10,  # Reduced to allow more splits\n",
                                     "                \"min_child_samples\": 5,  # Reduced to allow more granular predictions\n",
                                     "                \"feature_fraction\": 0.8,\n",
                                     "                \"bagging_fraction\": 0.8,\n",
                                     "                \"bagging_freq\": 1,\n",
                                     "                \"lambda_l1\": 0.05,  # Reduced regularization\n",
                                     "                \"lambda_l2\": 0.05,  # Reduced regularization\n",
                                     "                \"force_row_wise\": True,\n",
                                     "                \"seed\": 42\n",
                                     "            }\n",
                                     "            \n",
                                     "        # Get class distribution in training data\n",
                                     "        class_counts = np.bincount(y_train)\n",
                                     "        n_samples = len(y_train)\n",
                                     "        \n",
                                     "        # Calculate weight multipliers\n",
                                     "        weight_multipliers = np.array([\n",
                                     "            15.0 if label == 0 else  # H class\n",
                                     "            15.0 if label == 1 else  # L class\n",
                                     "            1.0  # None class\n",
                                     "            for label in range(num_class)\n",
                                     "        ])\n",
                                     "        \n",
                                     "        # Calculate balanced weights with multipliers\n",
                                     "        class_weights = (n_samples / (num_class * class_counts)) * weight_multipliers\n",
                                     "        sample_weights = class_weights[y_train]\n",
                                     "        \n",
                                     "        dtrain = lgb.Dataset(X_train, label=y_train, weight=sample_weights)\n",
                                     "        model = lgb.train(\n",
                                     "            params,\n",
                                     "            dtrain,\n",
                                     "            num_boost_round=1000,  # Reduced number of rounds\n",
                                     "            valid_sets=[dtrain],\n",
                                     "            callbacks=[\n",
                                     "                lgb.early_stopping(stopping_rounds=50),\n",
                                     "                lgb.log_evaluation(period=100)\n",
                                     "            ]\n",
                                     "        )\n",
                                     "        return model\n",
                                     "    except Exception as e:\n",
                                     "        print(\"[LGBM] Final training failed:\", repr(e))\n",
                                     "        return None\n",
                                     "\n",
                                     "# Enhanced logistic regression with class balancing\n",
                                     "def train_logreg(X_train, y_train):\n",
                                     "    # Use the same aggressive weighting as LightGBM\n",
                                     "    n_samples = len(y_train)\n",
                                     "    class_counts = np.bincount(y_train)\n",
                                     "    weight_multipliers = np.array([15.0, 15.0, 1.0])  # [H, L, None]\n",
                                     "    class_weights = (n_samples / (len(class_counts) * class_counts)) * weight_multipliers\n",
                                     "    class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
                                     "    \n",
                                     "    clf = LogisticRegression(\n",
                                     "        max_iter=2000,\n",
                                     "        multi_class=\u0027multinomial\u0027, \n",
                                     "        solver=\u0027saga\u0027,\n",
                                     "        class_weight=class_weight_dict,\n",
                                     "        C=0.3,\n",
                                     "        n_jobs=-1\n",
                                     "    )\n",
                                     "    clf.fit(X_train, y_train)\n",
                                     "    return clf\n",
                                     "\n",
                                     "majority_idx = int(np.bincount(y_enc).argmax())\n",
                                     "majority_label = le.inverse_transform([majority_idx])[0]\n",
                                     "\n",
                                     "# --- Train final model according to selected approach ---\n",
                                     "test_pred_enc = None\n",
                                     "\n",
                                     "if best_key == \u0027lgbm\u0027:\n",
                                     "    print(\"[FINAL] Training LightGBM on full data...\")\n",
                                     "    final_model = safe_train_lgbm_final(X, y_enc, num_class=len(le.classes_))\n",
                                     "    if final_model is None:\n",
                                     "        print(\"[FINAL] LGBM training failed; falling back to logistic regression.\")\n",
                                     "        logreg = train_logreg(X, y_enc)\n",
                                     "        pred_probs = logreg.predict_proba(X_test)\n",
                                     "        \n",
                                     "        # Apply calibrated thresholds for minority classes\n",
                                     "        none_idx = np.where(le.classes_ == \u0027None\u0027)[0][0]\n",
                                     "        h_idx = np.where(le.classes_ == \u0027H\u0027)[0][0]\n",
                                     "        l_idx = np.where(le.classes_ == \u0027L\u0027)[0][0]\n",
                                     "        \n",
                                     "        # More aggressive thresholds for minority classes\n",
                                     "        none_threshold = 0.8  # Higher threshold for None class\n",
                                     "        hl_threshold = 0.2   # Lower threshold for H/L classes\n",
                                     "        \n",
                                     "        # Start with predictions based on max probability\n",
                                     "        test_pred_enc = np.argmax(pred_probs, axis=1)\n",
                                     "        \n",
                                     "        # Override predictions based on thresholds\n",
                                     "        none_probs = pred_probs[:, none_idx]\n",
                                     "        hl_probs = pred_probs[:, [h_idx, l_idx]]\n",
                                     "        max_hl_probs = np.max(hl_probs, axis=1)\n",
                                     "        \n",
                                     "        # Where None probability is very high, predict None\n",
                                     "        strong_none_mask = none_probs \u003e= none_threshold\n",
                                     "        test_pred_enc[strong_none_mask] = none_idx\n",
                                     "        \n",
                                     "        # Where H/L probabilities are significant, predict H/L\n",
                                     "        strong_hl_mask = max_hl_probs \u003e= hl_threshold\n",
                                     "        hl_preds = np.argmax(hl_probs[strong_hl_mask], axis=1)\n",
                                     "        test_pred_enc[strong_hl_mask] = [h_idx if x == 0 else l_idx for x in hl_preds]\n",
                                     "        \n",
                                     "        del logreg; gc.collect()\n",
                                     "    else:\n",
                                     "        try:\n",
                                     "            pred_probs = final_model.predict(X_test)\n",
                                     "            if pred_probs.ndim == 1:\n",
                                     "                pred_probs = pred_probs.reshape(-1, len(le.classes_))\n",
                                     "            \n",
                                     "            # Get indices for each class\n",
                                     "            none_idx = np.where(le.classes_ == \u0027None\u0027)[0][0]\n",
                                     "            h_idx = np.where(le.classes_ == \u0027H\u0027)[0][0]\n",
                                     "            l_idx = np.where(le.classes_ == \u0027L\u0027)[0][0]\n",
                                     "            \n",
                                     "            # Much more aggressive thresholds to favor H/L predictions\n",
                                     "            none_threshold = 0.95  # Very high threshold for None\n",
                                     "            hl_threshold = 0.05    # Very low threshold for H/L\n",
                                     "            \n",
                                     "            # Initial predictions based on probability thresholds\n",
                                     "            test_pred_enc = np.full(pred_probs.shape[0], none_idx, dtype=int)\n",
                                     "            \n",
                                     "            # Apply thresholding rules\n",
                                     "            none_probs = pred_probs[:, none_idx]\n",
                                     "            hl_probs = pred_probs[:, [h_idx, l_idx]]\n",
                                     "            max_hl_probs = np.max(hl_probs, axis=1)\n",
                                     "            \n",
                                     "            # Target distribution (based on expectation: ~3% H, ~3% L, ~94% None)\n",
                                     "            target_h_ratio = 0.03\n",
                                     "            target_l_ratio = 0.03\n",
                                     "            \n",
                                     "            # Calculate number of samples for each class\n",
                                     "            n_samples = len(test_pred_enc)\n",
                                     "            target_h = int(n_samples * target_h_ratio)\n",
                                     "            target_l = int(n_samples * target_l_ratio)\n",
                                     "            \n",
                                     "            # Sort samples by H and L probabilities\n",
                                     "            h_probs = pred_probs[:, h_idx]\n",
                                     "            l_probs = pred_probs[:, l_idx]\n",
                                     "            \n",
                                     "            # Get top H predictions\n",
                                     "            h_indices = np.argsort(h_probs)[-target_h:]\n",
                                     "            test_pred_enc[h_indices] = h_idx\n",
                                     "            \n",
                                     "            # Get top L predictions (excluding those already marked as H)\n",
                                     "            remaining_mask = ~np.isin(np.arange(n_samples), h_indices)\n",
                                     "            l_indices = np.argsort(l_probs[remaining_mask])[-target_l:]\n",
                                     "            test_pred_enc[remaining_mask][l_indices] = l_idx\n",
                                     "            \n",
                                     "            # Only predict None for samples with very high None probability\n",
                                     "            strong_none_mask = none_probs \u003e= none_threshold\n",
                                     "            # Don\u0027t override H/L predictions we just made\n",
                                     "            none_indices = np.where(~np.isin(np.arange(n_samples), np.concatenate([h_indices, l_indices])) \u0026 strong_none_mask)[0]\n",
                                     "            test_pred_enc[none_indices] = none_idx\n",
                                     "            \n",
                                     "            # Print prediction distribution\n",
                                     "            print(\"\\nPrediction Distribution:\")\n",
                                     "            for label, count in zip(le.classes_, np.bincount(test_pred_enc, minlength=len(le.classes_))):\n",
                                     "                print(f\"{label}: {count} ({count/len(test_pred_enc)*100:.2f}%)\")\n",
                                     "            \n",
                                     "        except Exception as e:\n",
                                     "            print(\"[FINAL] LGBM prediction failed:\", repr(e))\n",
                                     "            test_pred_enc = np.full((X_test.shape[0],), majority_idx, dtype=int)\n",
                                     "        del final_model; gc.collect()\n",
                                     "\n",
                                     "elif best_key == \u0027hier\u0027:\n",
                                     "    print(\"[FINAL] Using hierarchical predictor for final predictions...\")\n",
                                     "    thr_cfg = cv_results[\u0027hier\u0027][\u0027cfg\u0027]\n",
                                     "    try:\n",
                                     "        test_pred_enc = hierarchical_predict_test(X, y_enc, X_test, classes_order, thr_cfg)\n",
                                     "    except Exception as e:\n",
                                     "        print(\"[FINAL] Hierarchical prediction failed:\", repr(e))\n",
                                     "        test_pred_enc = np.full((X_test.shape[0],), majority_idx, dtype=int)\n",
                                     "\n",
                                     "else:\n",
                                     "    print(\"[FINAL] Using logistic regression with balanced weights...\")\n",
                                     "    model = train_logreg(X, y_enc)\n",
                                     "    pred_probs = model.predict_proba(X_test)\n",
                                     "    \n",
                                     "    # Apply similar thresholding strategy\n",
                                     "    none_idx = np.where(le.classes_ == \u0027None\u0027)[0][0]\n",
                                     "    h_idx = np.where(le.classes_ == \u0027H\u0027)[0][0]\n",
                                     "    l_idx = np.where(le.classes_ == \u0027L\u0027)[0][0]\n",
                                     "    \n",
                                     "    # Initial predictions based on max probability\n",
                                     "    test_pred_enc = np.argmax(pred_probs, axis=1)\n",
                                     "    \n",
                                     "    # Apply thresholds\n",
                                     "    none_probs = pred_probs[:, none_idx]\n",
                                     "    hl_probs = pred_probs[:, [h_idx, l_idx]]\n",
                                     "    max_hl_probs = np.max(hl_probs, axis=1)\n",
                                     "    \n",
                                     "    strong_none_mask = none_probs \u003e= 0.8\n",
                                     "    test_pred_enc[strong_none_mask] = none_idx\n",
                                     "    \n",
                                     "    strong_hl_mask = max_hl_probs \u003e= 0.2\n",
                                     "    hl_preds = np.argmax(hl_probs[strong_hl_mask], axis=1)\n",
                                     "    test_pred_enc[strong_hl_mask] = [h_idx if x == 0 else l_idx for x in hl_preds]\n",
                                     "    \n",
                                     "    del model; gc.collect()\n",
                                     "\n",
                                     "# Ensure encoded predictions match test length\n",
                                     "test_pred_enc = np.asarray(test_pred_enc).astype(int).reshape(-1)\n",
                                     "if test_pred_enc.size != X_test.shape[0]:\n",
                                     "    print(\"[FINAL] Prediction length mismatch; filling with majority label.\")\n",
                                     "    test_pred_enc = np.full((X_test.shape[0],), majority_idx, dtype=int)\n",
                                     "\n",
                                     "# Map encoded labels back to original label names\n",
                                     "try:\n",
                                     "    test_pred_labels = le.inverse_transform(test_pred_enc)\n",
                                     "except Exception as e:\n",
                                     "    print(\"[FINAL] Label inverse_transform failed:\", repr(e))\n",
                                     "    classes = np.asarray(le.classes_)\n",
                                     "    clipped = np.clip(test_pred_enc, 0, len(classes) - 1)\n",
                                     "    test_pred_labels = classes[clipped]\n",
                                     "\n",
                                     "# Clean labels to ensure only valid competition classes\n",
                                     "valid_labels = set([\u0027H\u0027, \u0027L\u0027, \u0027None\u0027])  # Only allow official competition classes\n",
                                     "labels_series = pd.Series(test_pred_labels, dtype=object)\n",
                                     "labels_series = labels_series.fillna(\u0027None\u0027)\n",
                                     "labels_series = labels_series.replace({None: \u0027None\u0027, \u0027nan\u0027: \u0027None\u0027, \u0027NaN\u0027: \u0027None\u0027, \u0027\u0027: \u0027None\u0027})\n",
                                     "labels_series = labels_series.astype(str)\n",
                                     "\n",
                                     "# Ensure only competition-valid labels\n",
                                     "invalid_mask = ~labels_series.isin(valid_labels)\n",
                                     "if invalid_mask.any():\n",
                                     "    print(f\"[FINAL] Replacing {invalid_mask.sum()} invalid labels with \u0027None\u0027\")\n",
                                     "    labels_series.loc[invalid_mask] = \u0027None\u0027\n",
                                     "\n",
                                     "test_pred_labels = labels_series.to_numpy(dtype=object)\n",
                                     "\n",
                                     "# Force minimum counts for H and L based on training distribution\n",
                                     "if len(test) \u003e= 100:  # Only apply for reasonably sized test sets\n",
                                     "    min_h_count = max(int(len(test) * 0.03), 1)  # At least 3% H\n",
                                     "    min_l_count = max(int(len(test) * 0.03), 1)  # At least 3% L\n",
                                     "    \n",
                                     "    current_counts = pd.Series(test_pred_labels).value_counts()\n",
                                     "    h_count = current_counts.get(\u0027H\u0027, 0)\n",
                                     "    l_count = current_counts.get(\u0027L\u0027, 0)\n",
                                     "    \n",
                                     "    # If we need more H or L predictions, convert some high-probability None predictions\n",
                                     "    none_indices = np.where(test_pred_labels == \u0027None\u0027)[0]\n",
                                     "    if len(none_indices) \u003e 0:\n",
                                     "        # Get probabilities for None samples\n",
                                     "        if best_key == \u0027lgbm\u0027 or best_key == \u0027hier\u0027:\n",
                                     "            none_probs = pred_probs[none_indices]\n",
                                     "            \n",
                                     "            # Add more H predictions if needed\n",
                                     "            if h_count \u003c min_h_count:\n",
                                     "                h_candidates = none_indices[np.argsort(none_probs[none_indices, h_idx])[::-1]]\n",
                                     "                n_convert = min(min_h_count - h_count, len(h_candidates))\n",
                                     "                test_pred_labels[h_candidates[:n_convert]] = \u0027H\u0027\n",
                                     "            \n",
                                     "            # Add more L predictions if needed\n",
                                     "            if l_count \u003c min_l_count:\n",
                                     "                l_candidates = none_indices[np.argsort(none_probs[none_indices, l_idx])[::-1]]\n",
                                     "                n_convert = min(min_l_count - l_count, len(l_candidates))\n",
                                     "                test_pred_labels[l_candidates[:n_convert]] = \u0027L\u0027\n",
                                     "\n",
                                     "assert len(test_pred_labels) == len(test), \"Prediction/row count mismatch after cleaning.\"\n",
                                     "\n",
                                     "# Create submission IDs safely\n",
                                     "if id_col is not None and id_col in test.columns:\n",
                                     "    submission_ids = pd.Series(test[id_col], copy=True)\n",
                                     "else:\n",
                                     "    print(f\"[FINAL] Warning: id_col \u0027{id_col}\u0027 not found in test; using index as id.\")\n",
                                     "    submission_ids = pd.Series(test.index, copy=True)\n",
                                     "\n",
                                     "if submission_ids.isna().any():\n",
                                     "    print(\"[FINAL] Found NaNs in submission ids; replacing with sequential indices.\")\n",
                                     "    na_mask = submission_ids.isna()\n",
                                     "    submission_ids.loc[na_mask] = np.arange(len(submission_ids))[na_mask]\n",
                                     "submission_ids = submission_ids.astype(int, errors=\u0027ignore\u0027)\n",
                                     "\n",
                                     "submission_col = sample_sub.columns[1] if sample_sub is not None else \u0027target\u0027\n",
                                     "id_key = id_col if (id_col is not None and id_col in test.columns) else (sample_sub.columns[0] if sample_sub is not None else \u0027id\u0027)\n",
                                     "\n",
                                     "# Create final submission\n",
                                     "submission = pd.DataFrame({id_key: submission_ids, submission_col: test_pred_labels})\n",
                                     "submission[id_key] = submission[id_key].astype(submission_ids.dtype)\n",
                                     "submission[submission_col] = submission[submission_col].astype(str)\n",
                                     "\n",
                                     "# Final validation that we only have competition-valid labels\n",
                                     "valid_comp_labels = {\u0027H\u0027, \u0027L\u0027, \u0027None\u0027}\n",
                                     "assert submission[submission_col].isin(valid_comp_labels).all(), \"Found non-competition labels in submission!\"\n",
                                     "\n",
                                     "submission = submission[[id_key, submission_col]]\n",
                                     "submission_path = Path(\u0027./submission.csv\u0027)\n",
                                     "submission.to_csv(submission_path, index=False)\n",
                                     "print(\u0027Saved submission to\u0027, submission_path.resolve())\n",
                                     "print(\u0027\\nSubmission Label Distribution:\u0027)\n",
                                     "label_counts = submission[submission_col].value_counts()\n",
                                     "print(label_counts)\n",
                                     "print(\u0027\\nPercentages:\u0027)\n",
                                     "print((label_counts / len(submission) * 100).round(2), \u0027%\u0027)\n",
                                     "\n",
                                     "# Calculate expected counts based on training distribution\n",
                                     "train_dist = y.value_counts(normalize=True)\n",
                                     "expected_test_counts = (train_dist * len(test)).round(0)\n",
                                     "print(\u0027\\nExpected counts based on training distribution:\u0027)\n",
                                     "print(expected_test_counts.astype(int))\n",
                                     "\n",
                                     "display(submission.head())"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "b1018fb9-2217-4517-93a4-c6db36d6f785",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Notes\n",
                                     "- Set `DATASET_NAME` if your Kaggle dataset has a different slug.\n",
                                     "- This notebook auto-detects columns (`class_label`, `id`, `ticker_id`, `t`) and adapts.\n",
                                     "- It performs ticker-aware CV when possible, tunes a hierarchical threshold to optimize accuracy, and falls back gracefully.\n",
                                     "- Output: `submission.csv` with columns `[id, class_label]`.\n"
                                 ]
                  }
              ],
    "metadata":  {
                     "kaggle":  {
                                    "accelerator":  "none",
                                    "dataSources":  [
                                                        {
                                                            "databundleVersionId":  13695648,
                                                            "sourceId":  114135,
                                                            "sourceType":  "competition"
                                                        }
                                                    ],
                                    "dockerImageVersionId":  31153,
                                    "isGpuEnabled":  false,
                                    "isInternetEnabled":  true,
                                    "language":  "python",
                                    "sourceType":  "notebook"
                                },
                     "kernelspec":  {
                                        "display_name":  "Python 3",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.11.13"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
